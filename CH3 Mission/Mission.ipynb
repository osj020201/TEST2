{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #랭체인 : LLM을 사용하여 애플리케이션 생성을 단순화 하도록 설계된 프레임 워크. 랭체인에서 ChatOpenAI를 가져온다.\n",
    "from langchain_core.messages import HumanMessage # 기본 추상화 메세지\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # PDF문서 읽어주는 Pypdfloader 설치\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"C:\\\\Users\\\\owner\\\\Desktop\\\\sparta\\\\본캠프\\\\AI\\\\Mission2\\\\CH3 Mission\\\\인공지능산업최신동향_2024년11월호.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter # 주어진 텍스트를 문자 단위로 분할하는 데 사용함\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, #각 청크의 최대 길이 최대 100자까지의 텍스트가 하나의 청크에 포함된다.\n",
    "    chunk_overlap=10, #인접한 청크 사이에 중복으로 포함될 문자의 수, 각 청크들은 연결부분에서 10자까지 중복가능\n",
    "    length_function=len, #청크의 길이를 계산하는 함수, len 함수는 문자열의 길이를 기반으로 청크 길이를 계산함\n",
    "    is_separator_regex=False, # False로 설정해서, 구분자로 정규식을 사용하지 않음.\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings #OpenAI의 API를 활용하여, 각 문서를 대응하는 임베딩 벡터로 변환\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS # FAISS = 대용량의 데이터 간의 유사도를 빠르게 계산해주는 유사도 검색 라이브러리\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings) #from_documents = 문서 리스트와 임베딩 함수를 사용하여 FAISS 벡터 저장소를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.base import VectorStore\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate #ChatPromptTemplate = 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는데 사용하는 라이브러리\n",
    "from langchain_core.runnables import RunnablePassthrough #RunnablePassthrough = 데이터를 전달하는 역할\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([  #ChatPromptTemplate.from_messages = 메서드를 사용하여 메시지 리스트로부터 ChatPromptTemplate 인스턴스를 생성하는 방식은 대화형 프롬프트를 생성\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_112252\\89854149.py:49: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "from langchain.chains import LLMChain #LLMChain = 프롬프트 템플릿을 LLM을 합쳐서 컴포넌트화 한 것, 입력값으로 문자열을 넣으면 프롬프트 템플릿에 의해서 프롬프트가 자동으로 완성되고, LLM 모델을 호출하여 텍스트 출력을 내주는 기능\n",
    "# 컴포넌트화 = 재사용이 가능한 각각의 독립된 모듈\n",
    "\n",
    "class SimplePassThrough:\n",
    "    def invoke(self, inputs, **kwargs): #invoke = Retriever의 주요 진입점으로, 관련 문서를 검색하는 데 사용 /kwargs = Retriever에 전달할 추가 인자\n",
    "        return inputs #inputs = 검색 쿼리 문자열 \n",
    "\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "    \n",
    "    def invoke(self, inputs): \n",
    "        # 문서 내용을 텍스트로 변환\n",
    "        if isinstance(inputs, list): # isinstance = 타입확인\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in inputs]) #.join = 매개변수로 들어온 리스트에 있는 요소 하나하나를 합쳐서 하나의 문자열로 바꾸어 반환\n",
    "        else:\n",
    "            context_text = inputs\n",
    "        \n",
    "        # 프롬프트 템플릿에 적용\n",
    "        formatted_prompt = self.prompt_template.format_messages( #format_messages = 사용자의 입력을 프롬프트에 동적으로 삽입하여, 최종적으로 대화형 상황을 반영한 메시지 리스트를 생성\n",
    "            context=context_text,\n",
    "            question=inputs.get(\"question\", \"\")\n",
    "        )\n",
    "        return formatted_prompt\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # 검색 수행\n",
    "        response_docs = self.retriever.get_relevant_documents(query)\n",
    "        return response_docs\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n",
    "\n",
    "# RAG 체인 설정\n",
    "rag_chain_debug = {\n",
    "    \"context\": RetrieverWrapper(retriever),\n",
    "    \"prompt\": ContextToPrompt(contextual_prompt),\n",
    "    \"llm\": model\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "\n",
      "답변:\n",
      "인공지능 산업 동향은 다음과 같습니다:\n",
      "\n",
      "1. **정책/법제**: \n",
      "   - 미국 민권위원회가 얼굴인식 기술 사용에 따른 민권 영향을 분석하고, 백악관 예산관리국은 책임 있는 AI 조달을 위한 지침을 발표했습니다. \n",
      "   - 유로폴은 법 집행에서 AI의 이점과 과제를 다룬 보고서를 발간하였고, OECD는 공공 부문의 AI 도입을 위한 G7 툴킷을 발표했습니다. \n",
      "   - 세계경제포럼은 생성AI 시대의 거버넌스 프레임워크를 제시했습니다.\n",
      "\n",
      "2. **기업/산업**: \n",
      "   - 벤처 투자에서 AI 스타트업에 대한 집중이 증가하고 있으며, 메타는 동영상 생성AI 도구와 멀티모달 AI 모델을 공개했습니다. \n",
      "   - 앨런AI 연구소는 성능이 뛰어난 오픈소스 LLM '몰모'를 공개했으며, 카카오는 새로운 AI 서비스 '카나나'를 선보였습니다.\n",
      "\n",
      "3. **기술/연구**: \n",
      "   - 2024년 노벨 물리학상과 화학상 수상자가 AI 관련 연구자들입니다. \n",
      "   - 구글 딥마인드는 반도체 칩 레이아웃을 설계하는 AI 모델 '알파칩'을 발표했습니다.\n",
      "\n",
      "4. **인력/교육**: \n",
      "   - 가트너는 AI 도입으로 인해 엔지니어링 인력의 80%가 역량 향상이 필요하다고 예측했습니다. \n",
      "   - AI 전문가의 73%는 2025년 중 이직을 고려하고 있으며, 생성AI가 인간 근로자를 대체할 가능성은 낮다고 분석되었습니다.\n",
      "\n",
      "전체적으로 AI 기술의 발전과 정책적 대응이 활발하며, 기업들은 AI 통합과 인력 개발에 집중하고 있습니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "인공지능(AI)은 컴퓨터 시스템이 인간과 유사한 방식으로 작업을 수행할 수 있도록 하는 기술입니다. AI는 데이터 분석, 문제 해결, 학습, 언어 이해, 이미지 인식 등 다양한 분야에서 활용됩니다. 최근 AI는 생성AI와 같은 새로운 형태로 발전하고 있으며, 이는 텍스트, 이미지, 비디오 등을 생성하는 능력을 갖추고 있습니다.\n",
      "\n",
      "AI의 발전은 많은 산업 분야에 영향을 미치고 있으며, 특히 기업과 정부는 AI 기술을 통해 효율성을 높이고 새로운 서비스를 개발하고 있습니다. 그러나 AI가 인간 근로자를 완전히 대체할 가능성은 낮다는 연구 결과도 있습니다. 예를 들어, 최근 조사에 따르면 생성AI는 이론적 지식을 제공하는 데 강점을 보이지만, 물리적 작업을 수행하는 데는 한계가 있습니다.\n",
      "\n",
      "인공지능 관련 주요 행사로는 NeurIPS, GenAI Summit, AI Summit Seoul 등이 있으며, 이들 행사는 AI 기술과 연구 성과를 공유하고 발전 방향을 논의하는 플랫폼 역할을 하고 있습니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "The question seems to be missing. Please provide a specific question related to the context, and I will be happy to assist you.\n",
      "========================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m질문을 입력하세요 : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 1. Retriever로 관련 문서 검색\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     response_docs \u001b[38;5;241m=\u001b[39m rag_chain_debug[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query})\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 챗봇 구동\n",
    "while True:\n",
    "    print(\"========================\")\n",
    "    query = input(\"질문을 입력하세요 : \")\n",
    "    \n",
    "    # 1. Retriever로 관련 문서 검색\n",
    "    response_docs = rag_chain_debug[\"context\"].invoke({\"question\": query})\n",
    "    \n",
    "    # # 2. 문서를 프롬프트로 변환\n",
    "    prompt_messages = rag_chain_debug[\"prompt\"].invoke({\n",
    "        \"context\": response_docs,\n",
    "        \"question\": query\n",
    "    })\n",
    "    \n",
    "    # # 3. LLM으로 응답 생성\n",
    "    response = rag_chain_debug[\"llm\"].invoke(prompt_messages)\n",
    "    \n",
    "    print(\"\\n답변:\")\n",
    "    print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
