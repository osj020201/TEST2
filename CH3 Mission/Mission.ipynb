{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #랭체인 : LLM을 사용하여 애플리케이션 생성을 단순화 하도록 설계된 프레임 워크. 랭체인에서 ChatOpenAI를 가져온다.\n",
    "from langchain_core.messages import HumanMessage # 기본 추상화 메세지\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # PDF문서 읽어주는 Pypdfloader 설치\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"C:\\\\Users\\\\owner\\\\Desktop\\\\sparta\\\\본캠프\\\\AI\\\\Mission2\\\\CH3 Mission\\\\인공지능산업최신동향_2024년11월호.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter # 주어진 텍스트를 문자 단위로 분할하는 데 사용함\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, \n",
    "    chunk_overlap=10, \n",
    "    length_function=len, \n",
    "    is_separator_regex=False, \n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk_size=100, #각 청크의 최대 길이 최대 100자까지의 텍스트가 하나의 청크에 포함된다.\n",
    "chunk_overlap=10, #인접한 청크 사이에 중복으로 포함될 문자의 수, 각 청크들은 연결부분에서 10자까지 중복가능\n",
    "length_function=len, #청크의 길이를 계산하는 함수, len 함수는 문자열의 길이를 기반으로 청크 길이를 계산함\n",
    "is_separator_regex=False, # False로 설정해서, 구분자로 정규식을 사용하지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings #OpenAI의 API를 활용하여, 각 문서를 대응하는 임베딩 벡터로 변환\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS # FAISS = 대용량의 데이터 간의 유사도를 빠르게 계산해주는 유사도 검색 라이브러리\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings) #from_documents = 문서 리스트와 임베딩 함수를 사용하여 FAISS 벡터 저장소를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.base import VectorStore\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate #ChatPromptTemplate = 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는데 사용하는 라이브러리\n",
    "from langchain_core.runnables import RunnablePassthrough #RunnablePassthrough = 데이터를 전달하는 역할\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([  #ChatPromptTemplate.from_messages = 메서드를 사용하여 메시지 리스트로부터 ChatPromptTemplate 인스턴스를 생성하는 방식은 대화형 프롬프트를 생성\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_112252\\89854149.py:49: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "from langchain.chains import LLMChain #LLMChain = 프롬프트 템플릿을 LLM을 합쳐서 컴포넌트화 한 것, 입력값으로 문자열을 넣으면 프롬프트 템플릿에 의해서 프롬프트가 자동으로 완성되고, LLM 모델을 호출하여 텍스트 출력을 내주는 기능\n",
    "# 컴포넌트화 = 재사용이 가능한 각각의 독립된 모듈\n",
    "\n",
    "class SimplePassThrough:\n",
    "    def invoke(self, inputs, **kwargs): #invoke = Retriever의 주요 진입점으로, 관련 문서를 검색하는 데 사용 /kwargs = Retriever에 전달할 추가 인자\n",
    "        return inputs #inputs = 검색 쿼리 문자열 \n",
    "\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "    \n",
    "    def invoke(self, inputs): \n",
    "        # 문서 내용을 텍스트로 변환\n",
    "        if isinstance(inputs, list): # isinstance = 타입확인\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in inputs]) #.join = 매개변수로 들어온 리스트에 있는 요소 하나하나를 합쳐서 하나의 문자열로 바꾸어 반환\n",
    "        else:\n",
    "            context_text = inputs\n",
    "        \n",
    "        # 프롬프트 템플릿에 적용\n",
    "        formatted_prompt = self.prompt_template.format_messages( #format_messages = 사용자의 입력을 프롬프트에 동적으로 삽입하여, 최종적으로 대화형 상황을 반영한 메시지 리스트를 생성\n",
    "            context=context_text,\n",
    "            question=inputs.get(\"question\", \"\")\n",
    "        )\n",
    "        return formatted_prompt\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # 검색 수행\n",
    "        response_docs = self.retriever.get_relevant_documents(query)\n",
    "        return response_docs\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n",
    "\n",
    "# RAG 체인 설정\n",
    "rag_chain_debug = {\n",
    "    \"context\": RetrieverWrapper(retriever),\n",
    "    \"prompt\": ContextToPrompt(contextual_prompt),\n",
    "    \"llm\": model\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG가 필요한 이유 : \n",
    "생성형 AI 시스템이 외부 정보 소스를 사용하여 보다 정확한 상황 인식 응답을 생성할 수 있도록 해주기 때문이다.\n",
    "만약에 RAG가 없다면 외부 정보 소스를 이용할 수 없으며, 정확한 상황 인식 응답 또한 할 수 없기 때문에 생성형 AI를 만들때 RAG가 꼭 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "\n",
      "답변:\n",
      "인공지능(AI)은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 문제 해결, 의사 결정 등을 수행할 수 있게 하는 기술입니다. 최근 보고서에 따르면, AI는 다양한 산업에서 활용되고 있으며, 특히 생성AI는 직무 기술과 관련된 이론적 지식을 제공하는 능력이 뛰어난 것으로 평가받고 있습니다. 그러나 물리적 작업 수행이 필요한 직무에서는 인간 근로자를 대체할 가능성이 낮다고 합니다. AI 기술의 발전은 기업과 정부의 정책에 큰 영향을 미치고 있으며, AI 관련 주요 행사와 연구도 활발히 진행되고 있습니다.\n",
      "========================\n",
      "\n",
      "답변:\n",
      "What is the main focus of the November 2024 issue of the SPRi AI Brief?\n",
      "========================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m질문을 입력하세요 : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 1. Retriever로 관련 문서 검색\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     response_docs \u001b[38;5;241m=\u001b[39m rag_chain_debug[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query})\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 챗봇 구동\n",
    "while True:\n",
    "    print(\"========================\")\n",
    "    query = input(\"질문을 입력하세요 : \")\n",
    "    \n",
    "    # 1. Retriever로 관련 문서 검색\n",
    "    response_docs = rag_chain_debug[\"context\"].invoke({\"question\": query})\n",
    "    \n",
    "    # # 2. 문서를 프롬프트로 변환\n",
    "    prompt_messages = rag_chain_debug[\"prompt\"].invoke({\n",
    "        \"context\": response_docs,\n",
    "        \"question\": query\n",
    "    })\n",
    "    \n",
    "    # # 3. LLM으로 응답 생성\n",
    "    response = rag_chain_debug[\"llm\"].invoke(prompt_messages)\n",
    "    \n",
    "    print(\"\\n답변:\")\n",
    "    print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
